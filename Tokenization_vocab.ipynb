{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39886f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt',quiet=True)# nltk est un outil de segmentation\n",
    "nltk.download('punkt_tab')\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3f6479",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Détection des phrases vs mots isolés (juste une vérification)\n",
    "dataset=pd.read_excel(r\"D:\\projet_darija\\data\\processed\\dataset_preview.xlsx\")\n",
    "   ##chaque entré est une phrase ou un mot \n",
    "dataset[\"n1_is_phrase\"]=dataset[\"n1\"].apply(lambda x:\" \"in str(x))\n",
    "dataset[\"fren_is_phrase\"]=dataset[\"fren\"].apply(lambda x:\" \"in str(x))\n",
    "   ##Compter le nombre des tokens \n",
    "dataset[\"n1_token_count\"]=dataset[\"n1\"].apply(lambda x:len(word_tokenize(str(x))))\n",
    "dataset[\"fren_token_count\"]=dataset[\"fren\"].apply(lambda x:len(word_tokenize(str(x))))\n",
    "   ## nombre de ligne avec mots isolés\n",
    "print(\"le nombre de ligne en mots isolés{Darija}\",(dataset[\"n1_token_count\"]==1).sum())\n",
    "print(\"le nombre de ligne en mots isolés{fancais}\",(dataset[\"fren_token_count\"]==1).sum())\n",
    "   ##nombre de ligne avec phrases\n",
    "print(\"le nombre de ligne avec des phrases {Darija}\",(dataset[\"n1_token_count\"]>1).sum())\n",
    "print(\"le nombre de ligne avec des phrases{fancais}\",(dataset[\"fren_token_count\"]>1).sum())\n",
    "   # Aperçu\n",
    "print(dataset[[\"n1\", \"n1_token_count\", \"n1_is_phrase\", \"fren\", \"fren_token_count\", \"fren_is_phrase\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_excel(r\"D:\\projet_darija\\data\\processed\\dataset_preview.xlsx\")\n",
    "def tokenization_Vocab(sentences):\n",
    "    tokenized=[]   # liste des phrases tokenisées\n",
    "    vocab=set()    # ensemble des mots uniques \n",
    "    ##{tokenized}: pour préserver le contexte \n",
    "    ##{vocab}: pour abstraire le lexique global sans répétition\n",
    "    for s in sentences:\n",
    "        tokens =word_tokenize(str(s).lower())\n",
    "        tokenized.append(tokens)\n",
    "        vocab.update(tokens)\n",
    "    \n",
    "   # Tokens artificiels\n",
    "   # <pad> : padding ,compléter les séquences (aient la même longueur)\n",
    "   # <star> : le début de la phrase \n",
    "   # <end> : signaler la fin\n",
    "   # <next_id> : le prochain mot recevra l'index 3\n",
    "    word2idx = {'<pad>': 0, '<start>': 1, '<end>': 2}\n",
    "    next_id = 3\n",
    "   \n",
    "   # Attribuer un index á chaque mot unique\n",
    "    for x in sorted(vocab):\n",
    "        if x not in word2idx:\n",
    "            word2idx[x] = next_id  # Pour l'entraînement du modele (entrée du modele)\n",
    "            next_id +=1\n",
    "   # Dictionnaire inverse\n",
    "    idx2word = {i : x for x, i in word2idx.items() }  # Pour l'interprétation et la génération (sortie du modele)\n",
    "    return tokenized, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# génération des vocabulaire Darija et fren\n",
    "tokenized_darija, word2idx_darija, idx2word_darija = tokenization_Vocab(dataset[\"n1\"])       \n",
    "tokenized_fren, word2idx_fren,idx2word_fren  = tokenization_Vocab(dataset[\"fren\"])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9133d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder les fichiers \n",
    "with open (\"word2idx_darija.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2idx_darija, f, ensure_ascii=False, indent=4)\n",
    "with open (\"idx2word_darija.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(idx2word_darija, f, ensure_ascii=False, indent=4)\n",
    "with open (\"word2idx_fren.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2idx_fren, f, ensure_ascii=False, indent=4)\n",
    "with open (\"idx2word_fren.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(idx2word_fren, f, ensure_ascii=False, indent=4)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
